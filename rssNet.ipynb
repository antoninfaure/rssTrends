{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "from unicodedata import category\n",
    "import json\n",
    "import os\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from datetime import date, datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_urls = [\n",
    "    \"http://www.lemonde.fr/rss/une.xml\",\n",
    "    \"https://www.bfmtv.com/rss/news-24-7/\",\n",
    "    \"http://www.lefigaro.fr/rss/figaro_actualites.xml\",\n",
    "    \"https://www.lexpress.fr/arc/outboundfeeds/rss/alaune.xml\",\n",
    "    \"https://www.francetvinfo.fr/titres.rss\",\n",
    "    \"https://www.la-croix.com/RSS\",\n",
    "    \"http://tempsreel.nouvelobs.com/rss.xml\",\n",
    "    \"http://www.lepoint.fr/rss.xml\",\n",
    "    \"https://feeds.leparisien.fr/leparisien/rss\",\n",
    "    \"https://www.europe1.fr/rss.xml\",\n",
    "    \"https://partner-feeds.20min.ch/rss/20minutes\",\n",
    "    \"https://www.afp.com/fr/actus/afp_actualite/792,31,9,7,33/feed\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(feed_urls):\n",
    "    news_list = pd.DataFrame(columns=('title', 'summary', 'img_url', 'link'))\n",
    "\n",
    "    for feed_url in feed_urls:\n",
    "        res = requests.get(feed_url)\n",
    "        feed = BeautifulSoup(res.content, features='xml')\n",
    "        \n",
    "        articles = feed.findAll('item')\n",
    "        for article in articles:\n",
    "            news = {\n",
    "                'title': None,\n",
    "                'summary': None,\n",
    "                'link': None,\n",
    "                'img_url': None\n",
    "            }\n",
    "            news['title'] = BeautifulSoup(article.find('title').get_text(), \"html\").get_text()\n",
    "            if (article.find('description')):\n",
    "                news['summary'] = BeautifulSoup(article.find('description').get_text(), \"html\").get_text()\n",
    "            if (article.find('content')):\n",
    "                news['img_url'] = article.find('content')['url']\n",
    "            if (article.find('link')):\n",
    "                news['link'] = article.find('link').get_text()\n",
    "            news_list = pd.concat([news_list, pd.DataFrame([news])], ignore_index=True)\n",
    "        \n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(docs, lang='fr'):\n",
    "    if (lang=='fr'):\n",
    "        nlp = spacy.load('fr_core_news_lg')\n",
    "    elif (lang=='en'):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Utility functions\n",
    "    punctuation_chars =  [\n",
    "        chr(i) for i in range(sys.maxunicode)\n",
    "        if category(chr(i)).startswith(\"P\")\n",
    "    ]\n",
    "    \n",
    "    lemma_docs = []\n",
    "    for doc in docs:\n",
    "        # Tokenize docs\n",
    "        tokenized_doc = nlp(doc)\n",
    "\n",
    "        # Lemmanize docs\n",
    "        lemma_doc = list(filter(lambda token: token.is_stop == False and token.pos_ in ['NOUN', 'PROPN','ADJ'] and token.lemma_ not in [*string.punctuation, *punctuation_chars], tokenized_doc))\n",
    "        lemma_doc = list(map(lambda tok: tok.lemma_, lemma_doc))\n",
    "        lemma_docs.append(lemma_doc)\n",
    "\n",
    "\n",
    "    def get_vocabulary_frequency(documents):\n",
    "        vocabulary = dict()\n",
    "        for doc in documents:\n",
    "            for word in doc:\n",
    "                if word in list(vocabulary.keys()):\n",
    "                    vocabulary[word] += 1\n",
    "                else:\n",
    "                    vocabulary[word] = 1\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    voc = get_vocabulary_frequency(lemma_docs)\n",
    "\n",
    "    return lemma_docs, voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphnet(docs, voc, min_freq=5):\n",
    "    \n",
    "    # Filter voc with min_freq\n",
    "    filtered_voc = dict(filter(lambda elem: elem[1] > min_freq, voc.items()))\n",
    "\n",
    "    dict_voc_id = dict()\n",
    "    for i, term in enumerate(filtered_voc):\n",
    "        dict_voc_id[term] = i\n",
    "    \n",
    "    # List bigrams (edges)\n",
    "    finder = nltk.BigramCollocationFinder.from_documents(docs)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigrams = list(finder.score_ngrams(bigram_measures.raw_freq))\n",
    "    min_freq = min(list(map(lambda x: x[1], bigrams)))\n",
    "    bigrams = list(map(lambda x: (x[0], x[1]/min_freq), bigrams))\n",
    "\n",
    "    # Filter the bigrams with filtered_voc elements and replace by id\n",
    "    filtered_bigrams = []\n",
    "    for bigram in bigrams:\n",
    "        if (bigram[0][0] in filtered_voc.keys() and bigram[0][1] in filtered_voc.keys()):\n",
    "            new_bigram = ( dict_voc_id[bigram[0][0]] , dict_voc_id[bigram[0][1]] )\n",
    "            filtered_bigrams.append((new_bigram, bigram[1]))\n",
    "\n",
    "    # Set nodes sizes\n",
    "    sizes = list(filtered_voc.values())\n",
    "\n",
    "    # Format data\n",
    "    nodes = []\n",
    "    for i, term in enumerate(filtered_voc.keys()):\n",
    "        nodes.append({\n",
    "            'id': i,\n",
    "            'label': term,\n",
    "            'size': sizes[i]\n",
    "        })\n",
    "    \n",
    "    edges = []\n",
    "    for i, edge in enumerate(filtered_bigrams):\n",
    "        (source, target) = edge[0]\n",
    "        edges.append({\n",
    "            'id': i,\n",
    "            'source': source,\n",
    "            'target': target,\n",
    "            'size': edge[1]\n",
    "        })\n",
    "\n",
    "    \n",
    "    # Write JSON files\n",
    "    output_file(nodes, 'nodes.json')\n",
    "\n",
    "    \n",
    "    output_file(edges, 'edges.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_file(data, filename):\n",
    "    path = f'./data/{date.today().strftime(\"%d-%m-%Y\")}'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    with open(f'{path}/{filename}', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = scrap(feed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, voc = process_text(news_list['title'], lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphnet(docs, voc, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics(docs, criterion='leverage', level=0.01):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(docs).transform(docs, sparse=True)\n",
    "    df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df, min_support=0.005, use_colnames=True, verbose=1)\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "    rules = association_rules(frequent_itemsets, metric =\"lift\", min_threshold = 1)\n",
    "    rules = rules.sort_values([criterion], ascending =[False])\n",
    "\n",
    "    rules = rules[rules[criterion] > level]\n",
    "\n",
    "    topics = []\n",
    "    for i in rules.index:\n",
    "        rule = rules.loc[i]\n",
    "        x = list(rule['antecedents'])\n",
    "        y = list(rule['consequents'])\n",
    "        terms = x + y\n",
    "        found_similar = False\n",
    "        delete_topics_ids = []\n",
    "        for i, topic in enumerate(topics):\n",
    "            sim = similarity(topic, terms)\n",
    "            if (similarity(topic, terms) > 0.2):\n",
    "                found_similar = True\n",
    "                new_topic = list(set(list(topic) + terms))\n",
    "                delete_topics_ids.append(i)\n",
    "                break\n",
    "        if (found_similar == False):\n",
    "            topics.append((tuple(terms)))\n",
    "        else:\n",
    "            topics = [x for i, x in enumerate(topics) if i not in delete_topics_ids]\n",
    "            topics.insert(min(delete_topics_ids), tuple(new_topic))\n",
    "\n",
    "    return topics, rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dates():\n",
    "    dates = [x for x in next(os.walk('./data'))[1]]\n",
    "    dates.sort(key=lambda date: datetime.strptime(date, \"%d-%m-%Y\"), reverse=True)\n",
    "    dates = [{\"name\": x} for x in dates]\n",
    "    with open(f'./data/list.json', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = json.dump(dates, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 49 combinations | Sampling itemset size 7653\n"
     ]
    }
   ],
   "source": [
    "topics, rules = find_topics(docs, 'leverage', 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kiev', 'Ukraine', 'ballon', 'guerre', 'russe', 'char', 'Russie'),\n",
       " ('Pierre',\n",
       "  'Palmade',\n",
       "  'accident',\n",
       "  'garde',\n",
       "  'vue',\n",
       "  'homme',\n",
       "  'humoriste',\n",
       "  'passager'),\n",
       " ('Pen', 'réforme', 'motion', 'retraite', 'marine', 'censure'),\n",
       " ('vue', 'humoriste', 'passager', 'garde'),\n",
       " ('dérive',\n",
       "  'audit',\n",
       "  'Noël',\n",
       "  'Graët',\n",
       "  'FFF',\n",
       "  'fonction',\n",
       "  'crise',\n",
       "  'comportement',\n",
       "  'incompatible'),\n",
       " ('Pierre',\n",
       "  'Palmade',\n",
       "  'pratique',\n",
       "  'affaire',\n",
       "  'chemsex',\n",
       "  'humoriste',\n",
       "  'passager'),\n",
       " ('Écosse', 'ministre', 'Nicola', 'Sturgeon', 'démission'),\n",
       " ('Palmade', 'accident', 'comédien', 'humoriste', 'Pierre'),\n",
       " ('Buffalo', 'an', 'raciste', 'tuerie', 'prison', 'vie'),\n",
       " ('russe', 'char', 'Kiev', 'Ukraine'),\n",
       " ('dérive', 'Noël', 'impossible', 'FFF', 'statu', 'comportement'),\n",
       " ('Amélie', 'Oudéa', 'Castéra'),\n",
       " ('Vuitton', 'Louis', 'Pharrell', 'Williams'),\n",
       " ('statu', 'impossible'),\n",
       " ('démembré', 'butte', 'Chaumont', 'corps'),\n",
       " ('Netflix', 'Salto', 'français'),\n",
       " ('streaming', 'plateforme'),\n",
       " ('fonction', 'dérive', 'comportement', 'incompatible'),\n",
       " ('fonction', 'audit', 'comportement', 'incompatible'),\n",
       " ('Bayern', 'PSG'),\n",
       " ('procès', 'incendie'),\n",
       " ('Turquie', 'séisme'),\n",
       " ('Graët', 'audit', 'dérive'),\n",
       " ('Palmade', 'affaire', 'pratique', 'chemsex'),\n",
       " ('fonction', 'incompatible', 'FFF'),\n",
       " ('Graët', 'comportement'),\n",
       " ('violence', 'sexuel'),\n",
       " ('russe', 'Kiev', 'ballon'),\n",
       " ('euro', 'million'),\n",
       " ('février', 'grève'),\n",
       " ('char', 'Ukraine', 'guerre', 'an'),\n",
       " ('national', 'assemblée', 'opposition', 'retraite'),\n",
       " ('passager', 'homme'),\n",
       " ('aide', 'Ukraine')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>(Ukraine)</td>\n",
       "      <td>(guerre)</td>\n",
       "      <td>0.043071</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>12.901261</td>\n",
       "      <td>0.039733</td>\n",
       "      <td>2.928839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>(guerre)</td>\n",
       "      <td>(Ukraine)</td>\n",
       "      <td>0.043071</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>12.901261</td>\n",
       "      <td>0.039733</td>\n",
       "      <td>5.243446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>(Pierre)</td>\n",
       "      <td>(Palmade)</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.360000</td>\n",
       "      <td>0.039270</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>(Palmade)</td>\n",
       "      <td>(Pierre)</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>21.360000</td>\n",
       "      <td>0.039270</td>\n",
       "      <td>7.990012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>(réforme)</td>\n",
       "      <td>(retraite)</td>\n",
       "      <td>0.039326</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.413793</td>\n",
       "      <td>0.037190</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>(retraite)</td>\n",
       "      <td>(réforme)</td>\n",
       "      <td>0.039326</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>18.413793</td>\n",
       "      <td>0.037190</td>\n",
       "      <td>3.482444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>(garde)</td>\n",
       "      <td>(vue)</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>29.564014</td>\n",
       "      <td>0.028949</td>\n",
       "      <td>16.458801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>(vue)</td>\n",
       "      <td>(garde)</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>29.564014</td>\n",
       "      <td>0.028949</td>\n",
       "      <td>16.458801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>(Palmade)</td>\n",
       "      <td>(accident)</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>20.025000</td>\n",
       "      <td>0.026687</td>\n",
       "      <td>2.425094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>(accident)</td>\n",
       "      <td>(Palmade)</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>20.025000</td>\n",
       "      <td>0.026687</td>\n",
       "      <td>15.250936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>(Palmade, accident)</td>\n",
       "      <td>(Pierre)</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>22.654545</td>\n",
       "      <td>0.025060</td>\n",
       "      <td>14.382022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>(Pierre)</td>\n",
       "      <td>(Palmade, accident)</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>22.654545</td>\n",
       "      <td>0.025060</td>\n",
       "      <td>2.672753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>(Pierre, accident)</td>\n",
       "      <td>(Palmade)</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.360000</td>\n",
       "      <td>0.024990</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>(Palmade)</td>\n",
       "      <td>(Pierre, accident)</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>21.360000</td>\n",
       "      <td>0.024990</td>\n",
       "      <td>2.213143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>(accident)</td>\n",
       "      <td>(Pierre, Palmade)</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>21.238636</td>\n",
       "      <td>0.024983</td>\n",
       "      <td>7.670412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             antecedents          consequents   support  confidence  \\\n",
       "158            (Ukraine)             (guerre)  0.043071    0.676471   \n",
       "159             (guerre)            (Ukraine)  0.043071    0.821429   \n",
       "84              (Pierre)            (Palmade)  0.041199    1.000000   \n",
       "85             (Palmade)             (Pierre)  0.041199    0.880000   \n",
       "272            (réforme)           (retraite)  0.039326    1.000000   \n",
       "273           (retraite)            (réforme)  0.039326    0.724138   \n",
       "233              (garde)                (vue)  0.029963    0.941176   \n",
       "232                (vue)              (garde)  0.029963    0.941176   \n",
       "86             (Palmade)           (accident)  0.028090    0.600000   \n",
       "87            (accident)            (Palmade)  0.028090    0.937500   \n",
       "538  (Palmade, accident)             (Pierre)  0.026217    0.933333   \n",
       "539             (Pierre)  (Palmade, accident)  0.026217    0.636364   \n",
       "537   (Pierre, accident)            (Palmade)  0.026217    1.000000   \n",
       "540            (Palmade)   (Pierre, accident)  0.026217    0.560000   \n",
       "541           (accident)    (Pierre, Palmade)  0.026217    0.875000   \n",
       "\n",
       "          lift  leverage  conviction  \n",
       "158  12.901261  0.039733    2.928839  \n",
       "159  12.901261  0.039733    5.243446  \n",
       "84   21.360000  0.039270         inf  \n",
       "85   21.360000  0.039270    7.990012  \n",
       "272  18.413793  0.037190         inf  \n",
       "273  18.413793  0.037190    3.482444  \n",
       "233  29.564014  0.028949   16.458801  \n",
       "232  29.564014  0.028949   16.458801  \n",
       "86   20.025000  0.026687    2.425094  \n",
       "87   20.025000  0.026687   15.250936  \n",
       "538  22.654545  0.025060   14.382022  \n",
       "539  22.654545  0.025060    2.672753  \n",
       "537  21.360000  0.024990         inf  \n",
       "540  21.360000  0.024990    2.213143  \n",
       "541  21.238636  0.024983    7.670412  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules.head(15)[['antecedents', 'consequents', 'support', 'confidence', 'lift', 'leverage', 'conviction']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(x, y):\n",
    "    count = 0\n",
    "    for a in x:\n",
    "        for b in y:\n",
    "            if (b == a):\n",
    "                count += 1\n",
    "    return count/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(trend, docs, threshold=0.3):\n",
    "    results = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        sim = similarity(trend, doc)\n",
    "        if (sim > threshold):\n",
    "            results.append((i, sim))\n",
    "    results = sorted(results, key=lambda x: -x[1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trends(topics, docs):\n",
    "    trends = []\n",
    "    for topic in topics:\n",
    "        similar_docs = find_similarities(topic, docs)\n",
    "        img = None\n",
    "        for doc in similar_docs:\n",
    "            if (news_list.iloc[doc[0]]['img_url']):\n",
    "                img = news_list.iloc[doc[0]]['img_url']\n",
    "                break\n",
    "        trends.append({\n",
    "            \"topic\": topic,\n",
    "            \"docs\": similar_docs,\n",
    "            \"img_url\": img\n",
    "        })\n",
    "    \n",
    "    return trends"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f93075eebf97bc5e18c38d1a54e461de353c7365fd9def5e44782928585336f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
