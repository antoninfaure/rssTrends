{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "from unicodedata import category\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_urls = [\n",
    "    \"http://www.lemonde.fr/rss/une.xml\",\n",
    "    \"https://www.bfmtv.com/rss/news-24-7/\",\n",
    "    \"https://www.liberation.fr/rss/\",\n",
    "    \"http://www.lefigaro.fr/rss/figaro_actualites.xml\",\n",
    "    \"https://www.franceinter.fr/rss\",\n",
    "    \"https://www.lexpress.fr/arc/outboundfeeds/rss/alaune.xml\",\n",
    "    \"https://www.francetvinfo.fr/titres.rss\",\n",
    "    \"https://www.la-croix.com/RSS\",\n",
    "    \"http://tempsreel.nouvelobs.com/rss.xml\",\n",
    "    \"http://www.lepoint.fr/rss.xml\",\n",
    "    \"https://www.france24.com/fr/rss\",\n",
    "    \"https://feeds.leparisien.fr/leparisien/rss\",\n",
    "    \"https://www.ouest-france.fr/rss/une\",\n",
    "    \"https://www.europe1.fr/rss.xml\",\n",
    "    \"https://partner-feeds.20min.ch/rss/20minutes\",\n",
    "    \"https://www.afp.com/fr/actus/afp_actualite/792,31,9,7,33/feed\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(feed_urls):\n",
    "    news_list = pd.DataFrame(columns=('title', 'summary'))\n",
    "\n",
    "    for feed_url in feed_urls:\n",
    "        res = requests.get(feed_url)\n",
    "        feed = BeautifulSoup(res.content, features='xml')\n",
    "\n",
    "        articles = feed.findAll('item')       \n",
    "        for article in articles:\n",
    "            title = BeautifulSoup(article.find('title').get_text(), \"html\").get_text()\n",
    "            summary = \"\"\n",
    "            if (article.find('description')):\n",
    "                summary = BeautifulSoup(article.find('description').get_text(), \"html\").get_text()\n",
    "            news_list.loc[len(news_list)] = [title, summary]\n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(docs, lang='fr'):\n",
    "    if (lang=='fr'):\n",
    "        nlp = spacy.load('fr_core_news_sm')\n",
    "    elif (lang=='en'):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Utility functions\n",
    "    punctuation_chars =  [\n",
    "        chr(i) for i in range(sys.maxunicode)\n",
    "        if category(chr(i)).startswith(\"P\")\n",
    "    ]\n",
    "    def tokenize(text):\n",
    "        text = \"\".join(list(filter(lambda x: x not in [*string.punctuation, *punctuation_chars], text)))\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        words = list(filter(lambda x: x not in [stopwords.words('english') + stopwords.words('french')], tokens))\n",
    "        return list(map(lambda x: x.lower(), words))\n",
    "\n",
    "    def preprocess_text(documents):\n",
    "        docs = list(map(lambda doc: tokenize(doc), documents))\n",
    "        return docs\n",
    "    \n",
    "    # Clean and tokenize docs\n",
    "    tokenized_docs = preprocess_text(docs)\n",
    "    \n",
    "    # Lemmanize docs\n",
    "    def lemmanize(doc):\n",
    "        doc = list(filter(lambda token: token.lemma_ not in nlp.Defaults.stop_words, doc))\n",
    "        return list(map(lambda token: token.lemma_, doc))\n",
    "\n",
    "    lemma_docs = list(map(lambda doc: lemmanize(nlp(\" \".join(doc))), tokenized_docs))\n",
    "    def get_vocabulary_frequency(documents):\n",
    "        vocabulary = dict()\n",
    "        for doc in documents:\n",
    "            for word in doc:\n",
    "                if word in list(vocabulary.keys()):\n",
    "                    vocabulary[word] += 1\n",
    "                else:\n",
    "                    vocabulary[word] = 1\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    voc = get_vocabulary_frequency(lemma_docs)\n",
    "\n",
    "    return lemma_docs, voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphnet(docs, voc, min_freq=5, output_url='graph.html'):\n",
    "    \n",
    "    # Filter voc with min_freq\n",
    "    filtered_voc = dict(filter(lambda elem: elem[1] > min_freq, voc.items()))\n",
    "\n",
    "    dict_voc_id = dict()\n",
    "    for i, term in enumerate(filtered_voc):\n",
    "        dict_voc_id[term] = i\n",
    "    \n",
    "    # List bigrams (edges)\n",
    "    finder = nltk.BigramCollocationFinder.from_documents(docs)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigrams = list(finder.score_ngrams(bigram_measures.raw_freq))\n",
    "    bigrams = list(map(lambda x: x[0], bigrams))\n",
    "\n",
    "    # Filter the bigrams with filtered_voc elements and replace by id\n",
    "    bigrams = list(filter(lambda x: x[0] in filtered_voc.keys() and x[1] in filtered_voc.keys(), bigrams))\n",
    "    bigrams = list(map(lambda x: (dict_voc_id[x[0]], dict_voc_id[x[1]]), bigrams))\n",
    "\n",
    "    # Set nodes sizes\n",
    "    sizes = list(filtered_voc.values())\n",
    "\n",
    "    # Format data\n",
    "    nodes = []\n",
    "    for i, term in enumerate(filtered_voc.keys()):\n",
    "        nodes.append({\n",
    "            'id': i,\n",
    "            'label': term,\n",
    "            'size': sizes[i]\n",
    "        })\n",
    "    \n",
    "    edges = []\n",
    "    for i, edge in enumerate(bigrams):\n",
    "        (source, target) = edge\n",
    "        edges.append({\n",
    "            'id': i,\n",
    "            'source': source,\n",
    "            'target': target\n",
    "        })\n",
    "\n",
    "    \n",
    "    # Write JSON files\n",
    "    with open('nodes.json', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = json.dump(nodes, f, ensure_ascii=False)\n",
    "\n",
    "    \n",
    "    with open('edges.json', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = json.dump(edges, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = scrap(feed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tk news\n"
     ]
    }
   ],
   "source": [
    "docs, voc = process_text(news_list['summary'], lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphnet(docs, voc, min_freq=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f93075eebf97bc5e18c38d1a54e461de353c7365fd9def5e44782928585336f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
