{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "from unicodedata import category\n",
    "import json\n",
    "import os\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from datetime import date, datetime\n",
    "import warnings\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_urls = [\n",
    "    \"http://www.lemonde.fr/rss/une.xml\",\n",
    "    \"https://www.bfmtv.com/rss/news-24-7/\",\n",
    "    \"http://www.lefigaro.fr/rss/figaro_actualites.xml\",\n",
    "    \"https://www.lexpress.fr/arc/outboundfeeds/rss/alaune.xml\",\n",
    "    \"https://www.francetvinfo.fr/titres.rss\",\n",
    "    \"https://www.la-croix.com/RSS\",\n",
    "    \"http://tempsreel.nouvelobs.com/rss.xml\",\n",
    "    \"http://www.lepoint.fr/rss.xml\",\n",
    "    \"https://feeds.leparisien.fr/leparisien/rss\",\n",
    "    \"https://www.europe1.fr/rss.xml\",\n",
    "    \"https://partner-feeds.20min.ch/rss/20minutes\",\n",
    "    \"https://www.afp.com/fr/actus/afp_actualite/792,31,9,7,33/feed\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(feed_urls):\n",
    "    news_list = pd.DataFrame(columns=('title', 'summary', 'img_url', 'link'))\n",
    "\n",
    "    for feed_url in feed_urls:\n",
    "        res = requests.get(feed_url)\n",
    "        feed = BeautifulSoup(res.content, features='xml')\n",
    "        \n",
    "        articles = feed.findAll('item')\n",
    "        for article in articles:\n",
    "            news = {\n",
    "                'title': None,\n",
    "                'summary': None,\n",
    "                'link': None,\n",
    "                'img_url': None\n",
    "            }\n",
    "            news['title'] = BeautifulSoup(article.find('title').get_text(), \"html\").get_text()\n",
    "            if (article.find('description')):\n",
    "                news['summary'] = BeautifulSoup(article.find('description').get_text(), \"html\").get_text()\n",
    "            if (article.find('content')):\n",
    "                news['img_url'] = article.find('content')['url']\n",
    "            if (article.find('link')):\n",
    "                news['link'] = article.find('link').get_text()\n",
    "            news_list = pd.concat([news_list, pd.DataFrame([news])], ignore_index=True)\n",
    "        \n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(docs, lang='fr'):\n",
    "    if (lang=='fr'):\n",
    "        nlp = spacy.load('fr_core_news_lg')\n",
    "    elif (lang=='en'):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Utility functions\n",
    "    punctuation_chars =  [\n",
    "        chr(i) for i in range(sys.maxunicode)\n",
    "        if category(chr(i)).startswith(\"P\")\n",
    "    ]\n",
    "    \n",
    "    lemma_docs = []\n",
    "    for doc in docs:\n",
    "        # Tokenize docs\n",
    "        tokenized_doc = nlp(doc)\n",
    "\n",
    "        # Lemmanize docs\n",
    "        lemma_doc = list(filter(lambda token: token.is_stop == False and token.pos_ in ['NOUN', 'PROPN','ADJ'] and token.lemma_ not in [*string.punctuation, *punctuation_chars], tokenized_doc))\n",
    "        lemma_doc = list(map(lambda tok: tok.lemma_.lower(), lemma_doc))\n",
    "        lemma_docs.append(lemma_doc)\n",
    "\n",
    "\n",
    "    def get_vocabulary_frequency(documents):\n",
    "        vocabulary = dict()\n",
    "        for doc in documents:\n",
    "            for word in doc:\n",
    "                if word in list(vocabulary.keys()):\n",
    "                    vocabulary[word] += 1\n",
    "                else:\n",
    "                    vocabulary[word] = 1\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    voc = get_vocabulary_frequency(lemma_docs)\n",
    "\n",
    "    return lemma_docs, voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphnet(docs, voc, min_freq=5):\n",
    "    \n",
    "    # Filter voc with min_freq\n",
    "    filtered_voc = dict(filter(lambda elem: elem[1] > min_freq, voc.items()))\n",
    "\n",
    "    dict_voc_id = dict()\n",
    "    for i, term in enumerate(filtered_voc):\n",
    "        dict_voc_id[term] = i\n",
    "    \n",
    "    # List bigrams (edges)\n",
    "    finder = nltk.BigramCollocationFinder.from_documents(docs)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigrams = list(finder.score_ngrams(bigram_measures.raw_freq))\n",
    "    min_freq = min(list(map(lambda x: x[1], bigrams)))\n",
    "    bigrams = list(map(lambda x: (x[0], x[1]/min_freq), bigrams))\n",
    "\n",
    "    # Filter the bigrams with filtered_voc elements and replace by id\n",
    "    filtered_bigrams = []\n",
    "    for bigram in bigrams:\n",
    "        if (bigram[0][0] in filtered_voc.keys() and bigram[0][1] in filtered_voc.keys()):\n",
    "            new_bigram = ( dict_voc_id[bigram[0][0]] , dict_voc_id[bigram[0][1]] )\n",
    "            filtered_bigrams.append((new_bigram, bigram[1]))\n",
    "\n",
    "    # Set nodes sizes\n",
    "    sizes = list(filtered_voc.values())\n",
    "\n",
    "    # Format data\n",
    "    nodes = []\n",
    "    for i, term in enumerate(filtered_voc.keys()):\n",
    "        nodes.append({\n",
    "            'id': i,\n",
    "            'label': term,\n",
    "            'size': sizes[i]\n",
    "        })\n",
    "    \n",
    "    edges = []\n",
    "    for i, edge in enumerate(filtered_bigrams):\n",
    "        (source, target) = edge[0]\n",
    "        edges.append({\n",
    "            'id': i,\n",
    "            'source': source,\n",
    "            'target': target,\n",
    "            'size': edge[1]\n",
    "        })\n",
    "\n",
    "    \n",
    "    # Write JSON files\n",
    "    output_file(nodes, 'nodes.json')\n",
    "\n",
    "    \n",
    "    output_file(edges, 'edges.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_file(data, filename):\n",
    "    path = f'./data/{date.today().strftime(\"%d-%m-%Y\")}'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    with open(f'{path}/{filename}', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = scrap(feed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, voc = process_text(news_list['title'], lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphnet(docs, voc, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics(docs, criterion='leverage', level=0.01):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(docs).transform(docs, sparse=True)\n",
    "    df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df, min_support=0.005, use_colnames=True, verbose=1)\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "    rules = association_rules(frequent_itemsets, metric =\"lift\", min_threshold = 1)\n",
    "    rules = rules.sort_values([criterion], ascending =[False])\n",
    "\n",
    "    rules = rules[rules[criterion] > level]\n",
    "\n",
    "    topics = []\n",
    "    for i in rules.index:\n",
    "        rule = rules.loc[i]\n",
    "        x = list(rule['antecedents'])\n",
    "        y = list(rule['consequents'])\n",
    "        terms = x + y\n",
    "        found_similar = False\n",
    "        delete_topics_ids = []\n",
    "        for i, topic in enumerate(topics):\n",
    "            sim = similarity(topic, terms)\n",
    "            if (similarity(topic, terms) > 0.1):\n",
    "                found_similar = True\n",
    "                new_topic = list(set(list(topic) + terms))\n",
    "                delete_topics_ids.append(i)\n",
    "                break\n",
    "        if (found_similar == False):\n",
    "            topics.append((tuple(terms)))\n",
    "        else:\n",
    "            topics = [x for i, x in enumerate(topics) if i not in delete_topics_ids]\n",
    "            topics.insert(min(delete_topics_ids), tuple(new_topic))\n",
    "\n",
    "    return topics, rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dates():\n",
    "    dates = [x for x in next(os.walk('./data'))[1]]\n",
    "    dates.sort(key=lambda date: datetime.strptime(date, \"%d-%m-%Y\"), reverse=True)\n",
    "    dates = [{\"name\": x} for x in dates]\n",
    "    with open(f'./data/list.json', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = json.dump(dates, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(x, y):\n",
    "    count = 0\n",
    "    if (len(x) == 0 or len(y) == 0):\n",
    "        return 0\n",
    "    for a in x:\n",
    "        for b in y:\n",
    "            if (b == a):\n",
    "                count += 2\n",
    "\n",
    "    return count/(len(x)+len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(trend, docs, threshold=0.3):\n",
    "    results = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        sim = similarity(trend, doc)\n",
    "        if (sim > threshold):\n",
    "            results.append((i, sim))\n",
    "    results = sorted(results, key=lambda x: -x[1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trends(topics, docs):\n",
    "    trends = []\n",
    "    for topic in topics:\n",
    "        similar_docs = find_similarities(topic, docs)\n",
    "        img = None\n",
    "        for doc in similar_docs:\n",
    "            if (news_list.iloc[doc[0]]['img_url']):\n",
    "                img = news_list.iloc[doc[0]]['img_url']\n",
    "                break\n",
    "        trends.append({\n",
    "            \"topic\": topic,\n",
    "            \"docs\": similar_docs,\n",
    "            \"img_url\": img\n",
    "        })\n",
    "    \n",
    "    return trends"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary_frequency(corpus):\n",
    "    '''Select top-k (k = vocab_len) words in term of frequencies as vocabulary'''\n",
    "    voc2id = {}\n",
    "    count = dict()\n",
    "    for document in corpus:\n",
    "        if (len(document)>1):\n",
    "            for word in document:\n",
    "                word = word.lower()\n",
    "                if (word in count):\n",
    "                    count[word] += 1\n",
    "                else:\n",
    "                    count[word] = 1\n",
    "            \n",
    "    \n",
    "    sorted_count_by_freq = sorted(count.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    vocabulary = []\n",
    "    for i, x in enumerate(sorted_count_by_freq):\n",
    "        vocabulary.append(x[0])\n",
    "        voc2id[x[0]] = i\n",
    "    return vocabulary, voc2id\n",
    "\n",
    "vocab_freq, voc2id = create_vocabulary_frequency(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_word_cooccurence_matrix(voc2id, documents):\n",
    "    matrix = np.zeros((len(voc2id), len(voc2id)))\n",
    "    for document in documents:\n",
    "        if (len(document) > 1):\n",
    "            for word_i in document:\n",
    "                for word_j in document:\n",
    "                    if (word_i != word_j):\n",
    "                        matrix[voc2id[word_i], voc2id[word_j]] += 1\n",
    "    \n",
    "\n",
    "    return matrix/matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "wordcomat = construct_word_cooccurence_matrix(voc2id, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_similarity(b1, b2, vocab_freq, wordcomat, voc2id, k):\n",
    "    sim1 = []\n",
    "    for x in b1:\n",
    "        for w in top_k_words(x, vocab_freq, wordcomat, voc2id, k):\n",
    "            sim1.append(w[1])\n",
    "    sim2 = []\n",
    "    for x in b2:\n",
    "        for w in top_k_words(x, vocab_freq, wordcomat, voc2id, k):\n",
    "            sim2.append(w[1])\n",
    "    return similarity(sim1, sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_tops(word, vocab_freq, wordcomat, voc2id, k):\n",
    "    top_k = top_k_words(word, vocab_freq, wordcomat, voc2id, k)\n",
    "    results = []\n",
    "    for term in top_k:\n",
    "        if (word in list(map(lambda x: x[1], top_k_words(term[1], vocab_freq, wordcomat, voc2id, k)))):\n",
    "            results.append(term)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_words(word, vocab_freq, wordcomat, voc2id, k):\n",
    "    word_id = voc2id[word]\n",
    "    top_k_ids = np.argsort(wordcomat[word_id,:])[::-1][:k]\n",
    "    return [(wordcomat[word_id, i], vocab_freq[i]) for i in top_k_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics(docs, vocab_freq, wordcomat, voc2id, k=5):\n",
    "    topics = []\n",
    "    for i in range(100):\n",
    "        term = vocab_freq[i]\n",
    "        best_terms = find_similar_tops(term, vocab_freq, wordcomat, voc2id, k)\n",
    "        sims = np.zeros(len(topics))\n",
    "        if (len(best_terms) == 0):\n",
    "            continue\n",
    "        for j, topic in enumerate(topics):\n",
    "            sims[j] = bow_similarity([term] + [x[1] for x in best_terms], topic, vocab_freq, wordcomat,voc2id, k)\n",
    "        \n",
    "        raw_sim = bow_similarity([term], [x[1] for x in best_terms], vocab_freq, wordcomat,voc2id, k)\n",
    "        if (len(sims) > 0 and np.max(sims) > 0 and np.max(sims) > raw_sim):\n",
    "                best_topic_id = np.argmax(sims)\n",
    "                if (term not in topics[best_topic_id]):\n",
    "                    topics[best_topic_id].append(term)\n",
    "        else:\n",
    "            topics.append([term, *[x[1] for x in best_terms]])\n",
    "\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = find_topics(docs, vocab_freq, wordcomat,voc2id, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ukraine', 'guerre', 'russe', 'conflit', 'zelensky', 'kiev'],\n",
       " ['retraite', 'réforme', 'assemblée', 'national', 'débat', 'député'],\n",
       " ['palmade', 'pierre', 'accident', 'examen', 'involontaire', 'homicide'],\n",
       " ['mort', 'coupable', 'burkina', 'nichols', 'tyre', 'policier'],\n",
       " ['an', 'carrière', 'horizons', 'jour', 'long', 'séisme', 'million', 'euro'],\n",
       " ['paris', 'afp', 'particule', 'pollution', 'photo'],\n",
       " ['france', 'nucléaire', 'tournoi', 'avenir', 'xv', 'billet'],\n",
       " ['fin', 'réforme', 'assemblée', 'débat', 'gouvernement'],\n",
       " ['lfi', 'député', 'écologiste', 'prix'],\n",
       " ['français', 'cœur', 'monde', 'salade', 'pen'],\n",
       " ['femme', 'investigation', 'chaumont', 'butte', 'juge', 'instruction'],\n",
       " ['grève', 'mars', 'reconductible', 'raffinerie', 'cgt', 'libéral'],\n",
       " ['février', 'vendredi', 'lyon', 'journée', 'auxerre', 'psg'],\n",
       " ['bon', 'majordome', 'whatsapp', 'message', 'étoile'],\n",
       " ['nouveau', 'prostitution'],\n",
       " ['projet', 'bout', 'olivier', 'ps', 'travail', 'dussopt'],\n",
       " ['poutine', 'vladimir', 'tyran'],\n",
       " ['procès', 'barjols', 'prévenu', 'neuf', 'incendie'],\n",
       " ['borne', 'elisabeth', 'mépris', 'rn'],\n",
       " ['groupe', 'maximum', 'cotisation', 'carrière'],\n",
       " ['noir',\n",
       "  'victime',\n",
       "  'edf',\n",
       "  'personne',\n",
       "  'main',\n",
       "  'discrimination',\n",
       "  'homme',\n",
       "  'violence',\n",
       "  'sexuel'],\n",
       " ['sport', 'oms', 'meilleur', 'stansall', 'ben', 'enquête', 'image'],\n",
       " ['entreprise', 'plan', 'carbon', 'salarié'],\n",
       " ['clarisse', 'globe', 'vendée', 'populaire', 'banque', 'crémer'],\n",
       " ['turquie', 'séisme', 'syrie', 'rescapé', 'décombre'],\n",
       " ['bruce', 'willis', 'démence', 'temporal', 'fronto', 'dégénérescence'],\n",
       " ['conseil', 'hauts-de-seine', 'ministère', 'qpc', 'eric'],\n",
       " ['ciel', 'nord-américain', 'ovni', 'spéculation'],\n",
       " ['bout', 'projet', 'ps', 'faure', 'semaine'],\n",
       " ['vie', 'conducteur'],\n",
       " ['lr', 'sarkozy', 'lourd'],\n",
       " ['munich', 'conférence', 'prêt', 'conflit'],\n",
       " ['michel', 'ohayon', 'galerie', 'lafayette', 'gap'],\n",
       " ['train', 'déraillement', 'ohio', 'colère', 'face'],\n",
       " ['podcast', 'andré', 'archétype', 'basculement', 'complotist'],\n",
       " ['futur', 'européen', 'parisien', 'bhv'],\n",
       " ['âge', 'débat', 'vote', 'texte'],\n",
       " ['motion', 'national', 'censure', 'rassemblement'],\n",
       " ['président', 'mensonge', 'yaël', 'braun', 'pivet'],\n",
       " ['mondial', 'boxe', 'estelle']]"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f93075eebf97bc5e18c38d1a54e461de353c7365fd9def5e44782928585336f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
