{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "from unicodedata import category\n",
    "import json\n",
    "import os\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_urls = [\n",
    "    \"http://www.lemonde.fr/rss/une.xml\",\n",
    "    \"https://www.bfmtv.com/rss/news-24-7/\",\n",
    "    \"https://www.liberation.fr/rss/\",\n",
    "    \"http://www.lefigaro.fr/rss/figaro_actualites.xml\",\n",
    "    \"https://www.franceinter.fr/rss\",\n",
    "    \"https://www.lexpress.fr/arc/outboundfeeds/rss/alaune.xml\",\n",
    "    \"https://www.francetvinfo.fr/titres.rss\",\n",
    "    \"https://www.la-croix.com/RSS\",\n",
    "    \"http://tempsreel.nouvelobs.com/rss.xml\",\n",
    "    \"http://www.lepoint.fr/rss.xml\",\n",
    "    \"https://www.france24.com/fr/rss\",\n",
    "    \"https://feeds.leparisien.fr/leparisien/rss\",\n",
    "    \"https://www.ouest-france.fr/rss/une\",\n",
    "    \"https://www.europe1.fr/rss.xml\",\n",
    "    \"https://partner-feeds.20min.ch/rss/20minutes\",\n",
    "    \"https://www.afp.com/fr/actus/afp_actualite/792,31,9,7,33/feed\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(feed_urls):\n",
    "    news_list = pd.DataFrame(columns=('title', 'summary'))\n",
    "\n",
    "    for feed_url in feed_urls:\n",
    "        res = requests.get(feed_url)\n",
    "        feed = BeautifulSoup(res.content, features='xml')\n",
    "\n",
    "        articles = feed.findAll('item')       \n",
    "        for article in articles:\n",
    "            title = BeautifulSoup(article.find('title').get_text(), \"html\").get_text()\n",
    "            summary = \"\"\n",
    "            if (article.find('description')):\n",
    "                summary = BeautifulSoup(article.find('description').get_text(), \"html\").get_text()\n",
    "            news_list.loc[len(news_list)] = [title, summary]\n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(docs, lang='fr'):\n",
    "    if (lang=='fr'):\n",
    "        nlp = spacy.load('fr_core_news_lg')\n",
    "    elif (lang=='en'):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Utility functions\n",
    "    punctuation_chars =  [\n",
    "        chr(i) for i in range(sys.maxunicode)\n",
    "        if category(chr(i)).startswith(\"P\")\n",
    "    ]\n",
    "    \n",
    "    lemma_docs = []\n",
    "    for doc in docs:\n",
    "        # Tokenize docs\n",
    "        tokenized_doc = nlp(doc)\n",
    "\n",
    "        # Lemmanize docs\n",
    "        lemma_doc = list(filter(lambda token: token.is_stop == False and token.pos_ in ['NOUN', 'PROPN'] and token.lemma_ not in [*string.punctuation, *punctuation_chars], tokenized_doc))\n",
    "        lemma_doc = list(map(lambda tok: tok.lemma_, lemma_doc))\n",
    "        lemma_docs.append(lemma_doc)\n",
    "\n",
    "\n",
    "    def get_vocabulary_frequency(documents):\n",
    "        vocabulary = dict()\n",
    "        for doc in documents:\n",
    "            for word in doc:\n",
    "                if word in list(vocabulary.keys()):\n",
    "                    vocabulary[word] += 1\n",
    "                else:\n",
    "                    vocabulary[word] = 1\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    voc = get_vocabulary_frequency(lemma_docs)\n",
    "\n",
    "    return lemma_docs, voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphnet(docs, voc, min_freq=5, output_url='graph.html'):\n",
    "    \n",
    "    # Filter voc with min_freq\n",
    "    filtered_voc = dict(filter(lambda elem: elem[1] > min_freq, voc.items()))\n",
    "\n",
    "    dict_voc_id = dict()\n",
    "    for i, term in enumerate(filtered_voc):\n",
    "        dict_voc_id[term] = i\n",
    "    \n",
    "    # List bigrams (edges)\n",
    "    finder = nltk.BigramCollocationFinder.from_documents(docs)\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    bigrams = list(finder.score_ngrams(bigram_measures.raw_freq))\n",
    "    min_freq = min(list(map(lambda x: x[1], bigrams)))\n",
    "    bigrams = list(map(lambda x: (x[0], x[1]/min_freq), bigrams))\n",
    "\n",
    "    # Filter the bigrams with filtered_voc elements and replace by id\n",
    "    filtered_bigrams = []\n",
    "    for bigram in bigrams:\n",
    "        if (bigram[0][0] in filtered_voc.keys() and bigram[0][1] in filtered_voc.keys()):\n",
    "            new_bigram = ( dict_voc_id[bigram[0][0]] , dict_voc_id[bigram[0][1]] )\n",
    "            filtered_bigrams.append((new_bigram, bigram[1]))\n",
    "\n",
    "    # Set nodes sizes\n",
    "    sizes = list(filtered_voc.values())\n",
    "\n",
    "    # Format data\n",
    "    nodes = []\n",
    "    for i, term in enumerate(filtered_voc.keys()):\n",
    "        nodes.append({\n",
    "            'id': i,\n",
    "            'label': term,\n",
    "            'size': sizes[i]\n",
    "        })\n",
    "    \n",
    "    edges = []\n",
    "    for i, edge in enumerate(filtered_bigrams):\n",
    "        (source, target) = edge[0]\n",
    "        edges.append({\n",
    "            'id': i,\n",
    "            'source': source,\n",
    "            'target': target,\n",
    "            'size': edge[1]\n",
    "        })\n",
    "\n",
    "    \n",
    "    # Write JSON files\n",
    "    with open('nodes.json', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = json.dump(nodes, f, ensure_ascii=False)\n",
    "\n",
    "    \n",
    "    with open('edges.json', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = json.dump(edges, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "news_list = scrap(feed_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, voc = process_text(news_list['title'], lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphnet(docs, voc, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trends(docs, criterion='leverage'):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(docs).transform(docs, sparse=True)\n",
    "    df = pd.DataFrame.sparse.from_spmatrix(te_ary, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df, min_support=0.005, use_colnames=True, verbose=1)\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "    rules = association_rules(frequent_itemsets, metric =\"lift\", min_threshold = 1)\n",
    "    rules = rules.sort_values([criterion], ascending =[False])\n",
    "\n",
    "    rules = rules[rules[criterion] > 0.005]\n",
    "\n",
    "    trends = []\n",
    "    for i in rules.index:\n",
    "        rule = rules.loc[i]\n",
    "        x = list(rule['antecedents'])\n",
    "        y = list(rule['consequents'])\n",
    "        terms = x + y\n",
    "        ok = True\n",
    "        new_trend = terms\n",
    "        delete_trends_ids = []\n",
    "        for term in terms:\n",
    "            for i, trend in enumerate(trends):\n",
    "                if (term in trend):\n",
    "                    ok = False\n",
    "                    old_trend = new_trend\n",
    "                    new_trend = list(set(new_trend + list(trend)))\n",
    "                    #print(f'{old_trend} -> {new_trend}')\n",
    "                    delete_trends_ids.append(i)\n",
    "        if (ok == True):\n",
    "            trends.append((tuple(y + x)))\n",
    "        else:\n",
    "            trends = [x for i, x in enumerate(trends) if i not in delete_trends_ids]\n",
    "            trends.append(tuple(new_trend))\n",
    "\n",
    "    with open('trends.json', 'w', encoding='UTF8', newline='') as f:\n",
    "        writer = json.dump(trends, f, ensure_ascii=False)\n",
    "\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 30 combinations | Sampling itemset size 6542\n"
     ]
    }
   ],
   "source": [
    "trends = find_trends(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f93075eebf97bc5e18c38d1a54e461de353c7365fd9def5e44782928585336f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
